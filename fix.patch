diff --git a/scan.py b/scan.py
index 42db73875c206b71d2866dfb75eb747d064d4aca..fa2ad653828dc3a40764c1af195dfb6710aeef66 100644
--- a/scan.py
+++ b/scan.py
@@ -1,117 +1,111 @@
 import os
 import sys
 import logging
 import hashlib
 from datetime import datetime, timezone
 import pandas as pd
 import requests
 from tqdm import tqdm
 from concurrent.futures import ThreadPoolExecutor, as_completed
 
 # pylint: disable=broad-exception-caught
 
-_kline_cache = {}
 _sorted_klines_cache = {}
 
 def get_auth_headers() -> dict:
     api_key = os.getenv("BYBIT_API_KEY")
     if api_key:
         return {"X-BYBIT-API-KEY": api_key}
     return {}
 
 def get_tradeable_symbols_sorted_by_volume() -> list:
     url = "https://api.bybit.com/v5/market/tickers?category=linear"
     try:
         with tqdm(total=1, desc="Fetching symbols") as pbar:
             response = requests.get(url, headers=get_auth_headers(), timeout=10)
             response.raise_for_status()
             pbar.update(1)
         tickers = response.json().get("result", {}).get("list", [])
         filtered = [
             (item["symbol"], float(item.get("turnover24h", 0)))
             for item in tickers
             if item.get("symbol", "").endswith("USDT")
         ]
         sorted_filtered = sorted(filtered, key=lambda x: x[1], reverse=True)
         return sorted_filtered
     except Exception as e:
         print("[ERROR] Failed to fetch and sort symbols by volume: %s", e)
         return []
 
 def fetch_recent_klines(symbol: str, interval: str = "1", total: int = 30240) -> list:
-    if symbol in _kline_cache:
-        return _kline_cache[symbol][-total:]
 
     klines = []
     seen_chunks = set()
     now = datetime.now(timezone.utc)
     floored = now.replace(minute=(now.minute // 15) * 15, second=0, microsecond=0)
     end_time = int(floored.timestamp() * 1000)
     count = 1000
 
     while len(klines) < total:
         start_time = end_time - (count * 60 * 1000)
         url = (
             f"https://api.bybit.com/v5/market/kline?category=linear"
             f"&symbol={symbol}&interval={interval}&start={start_time}&limit={count}"
         )
         try:
             response = requests.get(url, headers=get_auth_headers(), timeout=10)
             response.raise_for_status()
             chunk = response.json().get("result", {}).get("list", [])
             if not chunk:
                 break
 
             chunk_key = hashlib.md5(str([row[0] for row in chunk]).encode()).hexdigest()
             if chunk_key in seen_chunks:
                 print("[DEBUG] Duplicate chunk detected, breaking early.")
                 return []
             seen_chunks.add(chunk_key)
             klines = chunk + klines
 
             end_time = start_time
         except Exception as e:
             print("[ERROR] Failed to fetch klines for %s: %s", symbol, e)
             break
 
     if len(klines) < 315:
         logger = logging.getLogger("volume_logger")
         logger.warning("%s: Only %d klines returned, skipping.", symbol, len(klines))
         return []
 
-    _kline_cache[symbol] = klines
     return klines[-total:]
 
 def calculate_volume_change(klines: list, block_size: int) -> float:
     try:
-        cache_key = (id(klines), block_size)
-        if cache_key in _sorted_klines_cache:
-            sorted_klines = _sorted_klines_cache[cache_key]
-        else:
-            sorted_klines = sorted(klines, key=lambda k: int(k[0]))
-            _sorted_klines_cache[cache_key] = sorted_klines
+        cache_key = id(klines)
+        if cache_key not in _sorted_klines_cache:
+            _sorted_klines_cache[cache_key] = sorted(klines, key=lambda k: int(k[0]))
+        sorted_klines = _sorted_klines_cache[cache_key]
 
         blocks = [
             sorted_klines[i:i + block_size]
             for i in range(0, len(sorted_klines) - (block_size - 1), block_size)
             if len(sorted_klines[i:i + block_size]) == block_size
         ]
         if len(blocks) < 21:
             return 0.0
 
         latest_block = blocks[-1]
         previous_blocks = blocks[-21:-1]
 
         sum_latest = sum(float(k[5]) for k in latest_block)
         avg_previous = sum(
             sum(float(k[5]) for k in block) for block in previous_blocks
         ) / len(previous_blocks)
 
         if avg_previous == 0:
             return 0.0
 
         return ((sum_latest - avg_previous) / avg_previous) * 100
     except Exception:
         return 0.0
 
 def setup_logging() -> logging.Logger:
diff --git a/test.py b/test.py
index e4c6ee68fcd99e8ce5b0ce3946c6b52549656ffd..8065ef5f3934151a2aa66c0b5f5325e02fee6fa3 100644
--- a/test.py
+++ b/test.py
@@ -56,25 +56,37 @@ def test_calculate_volume_change_valid():
     klines = one_block * 21
     pct = scan.calculate_volume_change(klines, 15)
     assert isinstance(pct, float)
     assert pct == 0.0
 
 def test_calculate_volume_change_insufficient_blocks():
     one_block = [[str(i), "", "", "", "", "1"] for i in range(15)]
     klines = one_block * 20
     pct = scan.calculate_volume_change(klines, 15)
     assert pct == 0.0
 
 def test_clean_existing_excels(tmp_path):
     dummy_file = tmp_path / "file.xlsx"
     dummy_file.write_text("data")
 
     with patch("scan.os.listdir", return_value=["file.xlsx"]), \
          patch("scan.os.remove") as mock_remove:
         scan.clean_existing_excels()
         mock_remove.assert_called_once_with("file.xlsx")
 
 def test_setup_logging():
     logger = scan.setup_logging()
     assert isinstance(logger, logging.Logger)
     assert logger.name == "volume_logger"
     assert logger.level == logging.INFO
+
+
+def test_calculate_volume_change_cache_reuse():
+    one_block = [[str(i), "", "", "", "", "1"] for i in range(15)]
+    klines = one_block * 21
+    scan._sorted_klines_cache.clear()
+    scan.calculate_volume_change(klines, 15)
+    # After first call the cache should have a single entry keyed by id(klines)
+    assert len(scan._sorted_klines_cache) == 1
+    scan.calculate_volume_change(klines, 60)
+    # Cache should still contain only one entry
+    assert len(scan._sorted_klines_cache) == 1
